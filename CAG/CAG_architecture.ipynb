{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ae5fe5-ce33-4ff6-9c9c-34fbf4cf9152",
      "metadata": {
        "id": "73ae5fe5-ce33-4ff6-9c9c-34fbf4cf9152",
        "outputId": "e8ad2237-aff5-4027-b6e0-b0d220206b45",
        "colab": {
          "referenced_widgets": [
            "b6ae4cfac5e443d1b313eb153616d12c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6ae4cfac5e443d1b313eb153616d12c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "IntSlider(value=5, description='Slider:', max=10)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import necessary packages\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "slider = widgets.IntSlider(value=5, min=0, max=10, step=1, description='Slider:')\n",
        "display(slider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3b4a45-e6ff-4129-a03f-bc054db65a2e",
      "metadata": {
        "id": "dc3b4a45-e6ff-4129-a03f-bc054db65a2e"
      },
      "outputs": [],
      "source": [
        "# Initialize the cache with a maximum size\n",
        "cache_size = 5  # Maximum number of cache entries\n",
        "cache = deque(maxlen=cache_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b811bc-7400-4ca9-819a-4c7dd7ec3266",
      "metadata": {
        "id": "e2b811bc-7400-4ca9-819a-4c7dd7ec3266"
      },
      "outputs": [],
      "source": [
        "# Function to add content to the cache\n",
        "def add_to_cache(text, cache):\n",
        "    cache.append(text)\n",
        "\n",
        "# Function to retrieve relevant content from the cache\n",
        "def retrieve_from_cache(query, cache, k=3):\n",
        "    return list(cache)[-k:]\n",
        "\n",
        "# Modified generate function with attention mask and padding\n",
        "def generate_with_cache(query, cache, model, tokenizer, max_length=1024, max_new_tokens=50):\n",
        "    # Retrieve relevant cached information\n",
        "    relevant_texts = retrieve_from_cache(query, cache)\n",
        "\n",
        "    # Combine the relevant cache text with the current query to form the complete context\n",
        "    context = \" \".join(relevant_texts) + \" \" + query\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode(context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    # Ensure the attention mask is set\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "\n",
        "    # Ensure the total input length is within the model's max length\n",
        "    if inputs.shape[1] > max_length:\n",
        "        inputs = inputs[:, -max_length:]\n",
        "        attention_mask = attention_mask[:, -max_length:]\n",
        "\n",
        "    # Check if there are any invalid token ids (out of vocab range)\n",
        "    if torch.any(inputs >= model.config.vocab_size):\n",
        "        raise ValueError(\"One or more tokens are out of vocabulary range.\")\n",
        "\n",
        "    # Generate the output using GPT-2\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_new_tokens=max_new_tokens, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "    # Decode the generated response\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2056e35-69bc-4cb7-b2d3-b594ab5a67dc",
      "metadata": {
        "id": "d2056e35-69bc-4cb7-b2d3-b594ab5a67dc"
      },
      "outputs": [],
      "source": [
        "# Adding some example text to the cache\n",
        "add_to_cache(\"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\", cache)\n",
        "add_to_cache(\"The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior.\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630613ef-6087-4648-a7c6-cd1c863f5ca8",
      "metadata": {
        "id": "630613ef-6087-4648-a7c6-cd1c863f5ca8",
        "outputId": "1fbc41b0-99ba-47d1-b010-cadfcd7999e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell me about the history and future of artificial intelligence, its applications, and current advancements.\n",
            "\n",
            "I'm a big fan of the idea of AI. I've been working on AI for a long time, but I'm not sure what it is yet. It's a very interesting idea, I think. But I don't think it's going to be a major breakthrough. There are a lot of things that are going on that I haven't really thought about yet, so I can't talk about them. So I'll just say that it will be interesting to see what happens. And I hope that we can get a better understanding of what's happening. We're going through a period of time where we're seeing a huge amount of progress in AI, which is a good thing. The question is, what are the implications of that?\n",
            ". . .\n",
            " (Laughter.)\n",
            ",\n",
            "-\n",
            "(Laughing.) I mean, it seems like there's some kind of a problem with that. You know, there are some things in the world that you can do with artificial intelligences that can be used to solve problems. One of them is that they can learn from experience. That's something that's been happening for some time. In fact, we've seen that with the human brain. Now, you know what? It can also learn. If you're a computer scientist, if you've ever worked on a machine, or you have a job, that machine can teach you a new way of thinking. What's interesting is how that works. When you learn something new, the machine learns. This is something we haven. People have been doing this for thousands of years. They've learned from the experience of their ancestors. How do you teach a human to learn? How can you do that in a way that is so different from what we have today? And that has been a really interesting question. Because it has to do, in some ways, with what you see in our brains. For example, when you look at the brain, they're very different. Their structure is very similar. Some of these things are very important. Others are not. These are things we don. Our brains are different, because we are so much more complex. As you go through the process of learning, your brain is going, \"Oh, this is what I want to know. Why am I doing it?\" And you start to think, oh, well, maybe I should do it. Maybe I shouldn't. Or maybe it should be something else. Whatever it was, whatever it may be, then you begin to wonder, is it really that important? Is it that much of an issue? I guess it depends on what the question was. Is there a reason why it wasn't important, why was it not important to me? Or is there something more important that was important for me to have done? So, for example. A lot has happened in this field over the last few years, where people have started to look into the possibility of using artificial neural networks to help us understand the way we think and how we act. Well, one of those things is the ability to understand what people are thinking and what they are feeling. Then you get to the point where you understand how they feel. Your brain can then learn to make that understanding. (Applause.) And then, of course, as you move through that process, people start looking at what is happening in their brains, how do they think about what their brain thinks. All of this has a profound effect on how you think in your head. Sometimes, even though you don\n",
            "\"know\" what your mind is thinking, sometimes you just don\"t know it at all. Even though it might be true, some people don \"know.\" They just think that way. Like, say, a person is saying, Oh, my God, he's really angry. He's angry because he thinks that he can help me. His brain doesn't know that, right? But it does know something about him. Right? (laughter)\n",
            ": . .\"\n",
            "\n",
            "\n",
            "\n",
            "The next question I wanted to ask you was about how the future will look. Do you believe that the next generation of computers will have the same capabilities as the ones that were used in human history?\n",
            "\n",
            ".\n",
            "\n",
            " (Brief pause.)\n",
            "\n",
            "\n",
            "\n",
            ",\n",
            "\n",
            "-\n",
            "\n",
            ":\n",
            "So, do we see a future where computers are able to read and write? Do we expect that to happen? Are we going back to a time when computers were able read, write, read? We don`t see that happening, though. Computers are still going in and out of existence. Are they going out and into space? They are just not going anywhere. No, not at this point. Not at that point, at least not yet at any point in time in history. At least, until we get there. Until we do. Let me just ask, are we really going backwards in that direction? If we\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"  # Replace with your actual model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the query\n",
        "query = \"Tell me about the history and future of artificial intelligence, its applications, and current advancements.\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(query, return_tensors=\"pt\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=1024,  # Control total response length\n",
        "    no_repeat_ngram_size=2,  # Reduce repetitive phrases\n",
        "    temperature=0.7,  # Control randomness\n",
        "    top_p=0.9,  # Use nucleus sampling for variety\n",
        "    top_k=50,  # Limit next token choices\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Decode and display result\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0656ef6-5515-46e2-bcb6-63ffdfb0222e",
      "metadata": {
        "id": "b0656ef6-5515-46e2-bcb6-63ffdfb0222e",
        "outputId": "8d4c5041-b6e6-46f6-dac7-e4f9680a0169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence (AI) refers to the simulation of human intelligence in machines. The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior. Tell me about the history and future of artificial intelligence, its applications, and current advancements.\n",
            "\n",
            "The Turing Test\n",
            ". . .\n",
            " (I'm not sure if I've ever heard of the Turing-test, but I'm sure it's a good one.)\n",
            ", a computer program that is programmed to perform a task. It is a program which is designed to do a certain task, such as a particular task or a specific task that requires a given amount of time. A Turing machine is an artificial intelligent system which can perform certain tasks. In the case of a Turing computer\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "# Load GPT-2 pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
        "\n",
        "# Ensure the pad token is set (GPT-2 does not have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize the cache with a maximum size\n",
        "cache_size = 5\n",
        "cache = deque(maxlen=cache_size)\n",
        "\n",
        "# Function to add content to the cache\n",
        "def add_to_cache(text, cache):\n",
        "    cache.append(text)\n",
        "\n",
        "# Function to retrieve relevant content from the cache\n",
        "def retrieve_from_cache(query, cache, k=3):\n",
        "    return list(cache)[-k:]  # Get the last k items\n",
        "\n",
        "# Function to generate text with cached context\n",
        "def generate_with_cache(query, cache, model, tokenizer, max_new_tokens=100):\n",
        "    # Retrieve relevant cached information\n",
        "    relevant_texts = retrieve_from_cache(query, cache)\n",
        "\n",
        "    # Combine the relevant cache text with the current query\n",
        "    context = \" \".join(relevant_texts) + \" \" + query\n",
        "\n",
        "    # Tokenize the input properly\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,  # Increased token count\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    # Decode the generated response\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Adding example text to the cache\n",
        "add_to_cache(\"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\", cache)\n",
        "add_to_cache(\"The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior.\", cache)\n",
        "\n",
        "# Query for AI history and future\n",
        "query = \"Tell me about the history and future of artificial intelligence, its applications, and current advancements.\"\n",
        "\n",
        "# Generate response using cache-aware function\n",
        "generated_text = generate_with_cache(query, cache, model, tokenizer)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cbfecd-c85b-43c0-ace0-ca26c31570d0",
      "metadata": {
        "id": "f1cbfecd-c85b-43c0-ace0-ca26c31570d0",
        "outputId": "8697ac9f-69d6-42e6-b188-d382f847cf20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence (AI) refers to the simulation of human intelligence in machines. The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior. Tell me about the history and future of artificial intelligence, its applications, and current advancements.\n",
            "\n",
            "\n",
            "I believe that AI is the next frontier in cognitive science and the development of computer programming languages. I want to share with you my hope for artificial intelligent computing. While it may be a bit of a long road ahead, I believe we are on the right path to come. AI can not only make machines smarter, it can also make them smarter.\n",
            "\n",
            "Technology is changing the way we think and think about all of the possible things that could be. Today, we have the technology to create a world where human beings could learn and grow smarter and smarter as they work together to improve the lives of people around the world. In this regard, robots will be the key to humanity's future. But it will also give us a chance to build a better world by bringing the values of real progress and ethics to all mankind. Here's how I think this could happen:\n",
            ". . .\n",
            "\n",
            ",\n",
            ",\n",
            "\n",
            " (1) Computers will never be able to understand human emotions. It may sound like a paradox, but the reality is that human emotion is a simple and often misunderstood system that we all believe can solve problems and overcome them. Computer emotions are the result of our own neural connections, which are often connected to other human systems. This makes it possible to train a computer to learn empathy. If you are familiar with the ethical law of kindness, you will see that it is very different from your typical human tendency to punish. Humans have a capacity for empathy, a process\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "# Load GPT-2 pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
        "\n",
        "# Ensure the pad token is set (GPT-2 does not have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize the cache with a maximum size\n",
        "cache_size = 5\n",
        "cache = deque(maxlen=cache_size)\n",
        "\n",
        "# Function to add content to the cache\n",
        "def add_to_cache(text, cache):\n",
        "    cache.append(text)\n",
        "\n",
        "# Function to retrieve relevant content from the cache\n",
        "def retrieve_from_cache(query, cache, k=3):\n",
        "    return list(cache)[-k:]  # Get the last k items\n",
        "\n",
        "# Function to generate text with cached context\n",
        "def generate_with_cache(query, cache, model, tokenizer, max_new_tokens=300):\n",
        "    # Retrieve relevant cached information\n",
        "    relevant_texts = retrieve_from_cache(query, cache)\n",
        "\n",
        "    # Combine the relevant cache text with the current query\n",
        "    context = \" \".join(relevant_texts) + \" \" + query\n",
        "\n",
        "    # Tokenize the input properly\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,  # Increased token count\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,  # Prevents repeating phrases\n",
        "            temperature=0.8,  # Controls randomness (higher = more diverse)\n",
        "            top_p=0.95,  # Nucleus sampling to filter low-probability tokens\n",
        "            top_k=100,  # Expands the selection of tokens for diversity\n",
        "            do_sample=True  # Enables sampling for more creative responses\n",
        "        )\n",
        "\n",
        "    # Decode the generated response\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Adding example text to the cache\n",
        "add_to_cache(\"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\", cache)\n",
        "add_to_cache(\"The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior.\", cache)\n",
        "\n",
        "# Query for AI history and future\n",
        "query = \"Tell me about the history and future of artificial intelligence, its applications, and current advancements.\"\n",
        "\n",
        "# Generate response using cache-aware function\n",
        "generated_text = generate_with_cache(query, cache, model, tokenizer)\n",
        "\n",
        "# Print the result\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac32950-df31-4192-8e98-b97d185a7d88",
      "metadata": {
        "id": "eac32950-df31-4192-8e98-b97d185a7d88",
        "outputId": "9535df18-ec5a-4327-9b34-1abfe5d2f92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Response:\n",
            " Tell me about the history and future of artificial intelligence.\n",
            " by the primary name is the term of the name of a name by using a list of how many of its name, is a term to be the \" name. ( to a \" the first to the number of being a form the other names, the word that is an name or the most commonly commonly used to which is ( a person to do, to name that the object by being by or by name the by word by that name for the world. that a word or in this, or a one or that in the man to describe the state in which the common to make or do to have the way of that which that as a man, by other name as the person by man is more than man by which he or man the day of man that man or men by any man man are we know who is to man in a male or he is man a general man and his name to me to he that he by men man as man who he in his work by him or him by a certain man of us by his or is by their name he/ man they are he was he have \" man his man was the country by one man he man for man which man about man when he has he are the United by he who has a father of men in man with him that one of his father in he the second man do man \"man by by f man through the men, man because he and man him man out man all man's man this man me man on man where he he said man. man from man over man men that they man whom he does man one by in all men are man (man is not man name man but man has man f by God man at man how he's self by to his other man after man other by, he would be man both men he men a he they're man more by whom man -- man us man himself man an man — man be he a thing man like man we man-man man what man said he, men who man between man my man\n",
            "\n",
            ", that we are men to him in \" men and the two man any of he about his men or woman by what he be by \" he all the he knows man God the the real man into man not he his word man also man am man their man saying the nation by of him to men they know man without man our man: man another man whether he not by and he could be a common man among man each man two men is he \", and that, other he of whom we men what they have man so man can be in men do he will man while he as he we have an person that I man named man either man actually man I know he person in that was man ... man according man it man if he being the fact man have he for his business by me by people by my day by another he to another name his own man no man even man does he co man people man most man c man - man/man the thing he can man than he I and men his person is in time man per man day man country man] man v man would man will he what that person, in him the president by time by thing by both man some man past manly man today man being man m man just man something man s man\" man dead man co he only man know it is of this name him a people are an he God in who are a day the year man across man since he do not his son by himself in what is his friend the work man de man only by I he it to they he f he work he know that are all by way man first man' man throughout man else man b man did he just he time in country that men the business man year by person man long man many men for he an God by this person of or God or his self the three man g man were man war man person he name \" to by all he may be \" his he their country in other men as \" is \" a fact by father a way he first he most of two by two he people he ( man whose man against man different man three men all of father the f a men we all his state by who they in one \" him men have a by it by matter he one thing that his country men men it the truth man may man been by for a God that about how man capital man say man alone man before man then man beyond man [ man father man matter the father by state man time that what I've man there man world by m we the woman a a or other that by co the or about all other to people to God and a business he this world man its man instead man ever manman in self man come man last man love man known man\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer (You can change 'gpt2' to another model)\n",
        "MODEL_NAME = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# GPT-2 doesn't have a padding token, so set it manually\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Define a simple in-memory cache (Dictionary-based)\n",
        "cache = {}\n",
        "\n",
        "def retrieve_from_cache(query, cache):\n",
        "    \"\"\"Retrieve relevant cached responses based on the query.\"\"\"\n",
        "    return cache.get(query, [])\n",
        "\n",
        "def store_in_cache(query, response, cache):\n",
        "    \"\"\"Store query-response pairs in cache for future retrieval.\"\"\"\n",
        "    cache[query] = response\n",
        "\n",
        "def generate_with_cache(query, cache, model, tokenizer, max_length=1024, max_new_tokens=300):\n",
        "    \"\"\"\n",
        "    Generates text using a Transformer model with caching.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The user input.\n",
        "        cache (dict): Cache storing previous query-response pairs.\n",
        "        model: Pre-trained transformer model.\n",
        "        tokenizer: Pre-trained tokenizer.\n",
        "        max_length (int): Maximum length of the input sequence.\n",
        "        max_new_tokens (int): Number of new tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant cached information\n",
        "    relevant_texts = retrieve_from_cache(query, cache)\n",
        "\n",
        "    # Combine cache with the current query\n",
        "    context = \" \".join(relevant_texts) + \" \" + query if relevant_texts else query\n",
        "\n",
        "    # Tokenize input and truncate if necessary\n",
        "    inputs = tokenizer.encode(context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    # Ensure the total input length is within the model's limit\n",
        "    max_length = min(inputs.shape[1], 1024)\n",
        "    inputs = inputs[:, -max_length:]\n",
        "\n",
        "    # Create attention mask\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "\n",
        "    # Create position IDs\n",
        "    position_ids = torch.arange(0, inputs.shape[1], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True,  # Enable sampling\n",
        "            temperature=0.8,  # Control randomness\n",
        "            top_p=0.95,  # Nucleus sampling\n",
        "            top_k=50  # Limit token choices\n",
        "        )\n",
        "\n",
        "    # Decode the generated response\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store in cache\n",
        "    store_in_cache(query, generated_text, cache)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example Query\n",
        "query = \"Tell me about the history and future of artificial intelligence.\"\n",
        "generated_text = generate_with_cache(query, cache, model, tokenizer)\n",
        "\n",
        "# Print the response\n",
        "print(\"\\nGenerated Response:\\n\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54adc583-b5a9-4602-9d89-43224e7e44a8",
      "metadata": {
        "id": "54adc583-b5a9-4602-9d89-43224e7e44a8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}